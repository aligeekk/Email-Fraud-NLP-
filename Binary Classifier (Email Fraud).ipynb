{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Import Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Explore the distribution of the Frauds VS Non-Frauds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x293a79ef908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's import the dataset\n",
    "fraud = pd.read_csv('Frauds.csv')\n",
    "\n",
    "# It's relatively simple, the email in string format, and an integer (0/1) indicating if it's fraudulent\n",
    "type(fraud)\n",
    "type(fraud['Text'][0])\n",
    "type(fraud['Class'][0])\n",
    "\n",
    "# Let's convert the 'Class' column to categorical to build our classifier later on\n",
    "fraud['Text'] = fraud['Text'].astype('category')\n",
    "\n",
    "# Let's take a look at the distribution of the fraud vs non-fraud emails\n",
    "fraud['Class'].value_counts().plot(kind='bar') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 2 classes are relatively balanced. \n",
    "\n",
    "## <font color = \"orange\"> If this was not the case, then we'd have to either over-sample the smaller group, or under-sample the larger group, depending on whether we have _sufficiently large number of data points for under-sampling to provide an accurate representation of the larger group_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Perform string tokenization, for removing meaningless stop words in each email, then join back the list into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Class</th>\n",
       "      <th>tokenizedTextCleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Supply Quality China's EXCLUSIVE dimensions at...</td>\n",
       "      <td>1</td>\n",
       "      <td>Supply Quality China's EXCLUSIVE dimensions Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>over. SidLet me know. Thx.</td>\n",
       "      <td>0</td>\n",
       "      <td>over. SidLet know. Thx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Friend,Greetings to you.I wish to accost ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Friend,Greetings you.I wish accost reques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....</td>\n",
       "      <td>1</td>\n",
       "      <td>MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not a surprising assessment from Embassy.</td>\n",
       "      <td>0</td>\n",
       "      <td>Not surprising assessment Embassy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monica -Huma Abedin &lt;Huma@clintonemail.com&gt;Tue...</td>\n",
       "      <td>0</td>\n",
       "      <td>Monica -Huma Abedin &lt;Huma@clintonemail.com&gt;Tue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pis print.H &lt;hrod17@clintonemail.com&gt;Thursday ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Pis print.H &lt;hrod17@clintonemail.com&gt;Thursday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dear Tom--H &lt;hrod17@clintonemail.com&gt;Friday De...</td>\n",
       "      <td>0</td>\n",
       "      <td>Dear Tom--H &lt;hrod17@clintonemail.com&gt;Friday De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Greetings from barrister Robert Williams=2CDea...</td>\n",
       "      <td>1</td>\n",
       "      <td>Greetings barrister Robert Williams=2CDear fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FYI. Thanks again for signing the book ---- an...</td>\n",
       "      <td>0</td>\n",
       "      <td>FYI. Thanks signing book ---- I hope get royal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Class  \\\n",
       "0  Supply Quality China's EXCLUSIVE dimensions at...      1   \n",
       "1                         over. SidLet me know. Thx.      0   \n",
       "2  Dear Friend,Greetings to you.I wish to accost ...      1   \n",
       "3  MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....      1   \n",
       "4          Not a surprising assessment from Embassy.      0   \n",
       "5  Monica -Huma Abedin <Huma@clintonemail.com>Tue...      0   \n",
       "6  Pis print.H <hrod17@clintonemail.com>Thursday ...      0   \n",
       "7  Dear Tom--H <hrod17@clintonemail.com>Friday De...      0   \n",
       "8  Greetings from barrister Robert Williams=2CDea...      1   \n",
       "9  FYI. Thanks again for signing the book ---- an...      0   \n",
       "\n",
       "                                tokenizedTextCleaned  \n",
       "0  Supply Quality China's EXCLUSIVE dimensions Un...  \n",
       "1                            over. SidLet know. Thx.  \n",
       "2  Dear Friend,Greetings you.I wish accost reques...  \n",
       "3  MR. CHEUNG PUIHANG SENG BANK LTD.DES VOEUX RD....  \n",
       "4                 Not surprising assessment Embassy.  \n",
       "5  Monica -Huma Abedin <Huma@clintonemail.com>Tue...  \n",
       "6  Pis print.H <hrod17@clintonemail.com>Thursday ...  \n",
       "7  Dear Tom--H <hrod17@clintonemail.com>Friday De...  \n",
       "8  Greetings barrister Robert Williams=2CDear fri...  \n",
       "9  FYI. Thanks signing book ---- I hope get royal...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize each email into a discrete list of words\n",
    "fraud['tokenizedText'] = fraud['Text'].str.split()\n",
    "\n",
    "# Observe that we've ONE null value in our tokenizedText column\n",
    "fraud[fraud['tokenizedText'].isnull() == True]\n",
    "\n",
    "# Observe that we've NO null value in our tokenizedText column\n",
    "fraud[fraud['Class'].isnull() == True]\n",
    "\n",
    "# Drop the single row containing NA in tokenizedText\n",
    "fraud = fraud.dropna()\n",
    "\n",
    "# Create a set of stopwords to optimize lookup time to O(1)\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Remove the stopwords \n",
    "fraud['tokenizedText'] = fraud['tokenizedText'].apply(lambda x: [item for item in x if item not in stopWords])\n",
    "\n",
    "# Join the words back into a single string\n",
    "fraud['tokenizedTextCleaned'] = fraud['tokenizedText'].str.join(' ')\n",
    "\n",
    "# Drop the tokenizedText column\n",
    "fraud = fraud.drop(axis = 1, columns = 'tokenizedText')\n",
    "\n",
    "# View the dataframe\n",
    "fraud.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"orange\"> Observe how many meaningless words have been removed from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Perform a basic train-test split and apply a simple logistic regression model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train-test split for the dataset\n",
    "X = fraud['tokenizedTextCleaned']\n",
    "y = fraud['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"orange\"> I've chosen to keep things very simple here, just to illustrate the idea\n",
    "# <font color = \"orange\"> We could've instead used a K-Fold Cross Validation if we wish to:\n",
    "## 1) Prevent overfitting (low bias (ie. very accurate on average) high variance models), which is especially common on high-dimensional datasets like our TF-IDF matrix later, since every single word is mapped as a feature for our logistic regresion to train on\n",
    "## 2) Wanted to tune the hyperparameters without using another validation set - this will further drop our training dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. We've to first pre-process our cleaned email strings via 3 steps:\n",
    "## 1) Convert the strings into a COUNT MATRIX, where every feature/column heading refers to a particular word, and each cell contains the word frequency in the string (ie. each document) <br>\n",
    "## 2) We then convert this Count Matrix into a TF-IDF Matrix (TF * IDF)\n",
    "\n",
    "# <font color = \"orange\"> Key Concept : Count Matrix ---> TF-IDF Matrix\n",
    "### (1) Term Frequency (Higher = More relevant word = More weight attached to it)\n",
    "### (a) Refers to the RELATIVE frequency that a word shows up in a PARTICULAR DOCUMENT (ie. email)\n",
    "### (b) This helps to take document size (ie. email length) out of the equation, this is similar to Standard Scaling (ie. normalizing) our numerical features (eg. AGE, INCOME) to avoid large absolute values from being considered as overly important during model fitting <br>\n",
    "\n",
    "### (2) Inverse Document Frequency (tells us how SPECIFIC the word is. The more specific it is to a document/email, the more weight we attach to it when calculating IDF values)\n",
    "### (a) Refers to the logarithm of the INVERSE of the term frequency across ALL DOCUMENTS (ie. all emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model\n",
      "Accuracy : 0.9823973176865046\n",
      "AUC : 0.9846152002760666\n",
      "\n",
      "Confusion Matrix\n",
      "[[1997    1]\n",
      " [  62 1519]]\n",
      "\n",
      "FNR = 1.73%\n"
     ]
    }
   ],
   "source": [
    "# Our preprocessing pipeline is :\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', LogisticRegression()),\n",
    "                  ])\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Fit and predict on the testing set\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Logistic Regression Model')\n",
    "print('Accuracy : %s' % accuracy_score(y_pred, y_test))\n",
    "print('AUC : %s' % roc_auc_score(y_pred, y_test))\n",
    "print()\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "\n",
    "# Check the False Negative Rate (this refers to the percentage of FRAUDULENT emails that our classifier FAILED TO CATCH)\n",
    "print('FNR = {0:.2f}%'.format(confusion_matrix(y_test, y_pred)[1][0] / sum(sum(confusion_matrix(y_test, y_pred))) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'orange'> That's weird... basic logistic regression models shouldn't have this kind of accuracy on high-dimensional text classification datasets like this. Head back to Kaggle to check other people's works : https://www.kaggle.com/llabhishekll/fraud-email-dataset/kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy is pretty horrible for assessing a classifier, we instead use a ROC for general evaluation, where the Area Under Curve (AUC) can be evaluated\n",
    "\n",
    "# Also, FNR seems to me to be the most important in assessing how good our classifier is. Ideally, FNR should be 0 to minimize losses to SocGen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"orange\"> Hence, note the dangerous 62 False Negatives (fraud emails classified as SAFE) that we have which could potentially lead to expensive data breaches\n",
    "    \n",
    "# <font color = \"orange\"> SOLUTION??? Simply lower the classifier's cutoff from the default 50% that is set. COMPROMISE??? We'll probably get a lot of false alarms. If we choose to rely on it, a SIMPLE WARNING can be presented to employees, instead of choosing to send the email straight to Spam/Trash, as we don't want to delete important client emails that were incorrectly flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"orange\"> F. Can we do better? Considering information leaks from investment banks could potentially cost millions in lost revenue or lawsuit damages, can we develop a better model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now try a RANDOM FOREST CLASSIFIER, which tends to avoid the pitfalls of overfitting in extremely high-dimensional datasets like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classification Model\n",
      "Accuracy : 0.9804414640961162\n",
      "AUC : 0.9819033382841333\n",
      "\n",
      "Confusion Matrix\n",
      "[[1986   12]\n",
      " [  58 1523]]\n",
      "\n",
      "FNR = 1.62%\n"
     ]
    }
   ],
   "source": [
    "# Our preprocessing pipeline is :\n",
    "rfc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', RandomForestClassifier()),\n",
    "               ])\n",
    "\n",
    "# We perform GridSearch during the CV Process to tune the model's hyperparameters as we train it\n",
    "# Let's just focus on the 3 most important ones, otherwise training on a regular PC will take too long \n",
    "n_estimators = [500, 1000]\n",
    "max_depth = [50, 100]\n",
    "min_samples_split = [5, 10]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'clf__n_estimators': n_estimators,\n",
    "               'clf__max_depth': max_depth,\n",
    "               'clf__min_samples_split': min_samples_split}\n",
    "\n",
    "# Start the Randomized Grid Search\n",
    "grid_search = RandomizedSearchCV(estimator = rfc,  param_distributions = random_grid, \n",
    "                                 n_iter = 3, cv = 2, n_jobs = -1, verbose = 0)\n",
    "\n",
    "# Fit the RFC model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Fit and predict on the testing set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "print('Random Forest Classification Model')\n",
    "print('Accuracy : %s' % accuracy_score(y_pred, y_test))\n",
    "print('AUC : %s' % roc_auc_score(y_pred, y_test))\n",
    "print()\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "\n",
    "# Check the False Positive Rate (this refers to the percentage of FRAUDULENT emails that our classifier FAILED TO CATCH)\n",
    "print('FNR = {0:.2f}%'.format(confusion_matrix(y_test, y_pred)[1][0] / sum(sum(confusion_matrix(y_test, y_pred))) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'orange'> Well, we did a little better on this... but still, due to a lack of computational power on my PC, we weren't able to exhaustively search a larger search space for our hyperparameters quickly, to yield an even better model with a lower FNR.\n",
    "    \n",
    "# Usual Steps :\n",
    "1) Perform a Random Search on many different hyperparameter combinations <br>\n",
    "2) Once the best has been selected, we narrow the combination range in the tuning grid <br>\n",
    "3) Apply this more specific tuning grid and perform an exhaustive Grid Search to go through EACH POSSIBLE COMBINATION <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. Final Thoughts & Possible Improvements\n",
    "## <font color = 'orange'> 1. Applying PCA to counter over-fitting on high-dimensional TF-IDF Matrix \n",
    "1) N-features = N-dimensions <br><br>\n",
    "2) Assume 2 features, hence 2 dimensions <br><br>\n",
    "3) In 2D Space : Think of PCA as a \"best-fit line\" through the points, and this line will now become the principal component, PC1, which explains for the largest variance (ie. scattering) of the points. This is also known as the eigenvector in PCA, and corresponds to a COMPLETELY NEW \"VARIABLE\", from a linear combination (eg X+Y) of the original 2 variables, X and Y. <br><br>\n",
    "4) We then draw a perpenticular to this eigenvector, PC2, which consequently accounts for the LEAST variation in the dataset, and hence can be dropped. <br><br>\n",
    "\n",
    "## <font color = 'orange'> 2. L1 regularization (to reduce overfitting on high dimensional datasets), which is extremely suitable for sparse vectors, such as the TF-IDF matrix we're running our classification algorithm on. L1 will completely remove the features/words that are not that important (ie. smallest TF-IDF values), whereas L2 will simply scale them down\n",
    "    \n",
    "## <font color = 'orange'> 3. Perform over-sampling (either through simply repeating points, or to apply clustering algorithms to generate SIMILAR SYNTHETIC DATA POINTS IN THE TF-IDF MATRIX for the smaller dataset) of the smaller dataset \n",
    "    \n",
    "## <font color = 'orange'> 4.Depending on the importance of FPR vs FNR, we can tweak our classifier's cutoff to focus on MINIMIZING ONE. My guess is FNR is much more detrimental to SocGen in terms of potential losses incurred.\n",
    "    \n",
    "## <font color = 'orange'> 4.Depending on the importance of FPR vs FNR, we can tweak our classifier's cutoff to focus on MINIMIZING ONE. My guess is FNR is much more detrimental to SocGen in terms of potential losses incurred.\n",
    "    \n",
    "## <font color = 'orange'> 5. I believe MLP classifiers will hit over 99% accuracy even on testing dataset. However, it can be very easy to overfit the training data if appropriate dropout layers aren't added when constructing the neural net, and also it is extremely data-hungry, which shoudln't be a problem for big banks like yourself, but smaller banks who wish to implement such a system will find it difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H. Questions for SocGen \n",
    "\n",
    "1. @ Ricky : As a IT Fraud Manager, are these traditional ML models still used extensively, or have neural nets dominated the industry? It seems like everyone everywhere is using all these deep learning methods, do you think there is still a place for traditional ML models like the ones I used in this project?\n",
    "\n",
    "2. Are the models that I've developed in this script too simple for use in real life? What kind of improvements should I make in making them better?\n",
    "\n",
    "3. May I know what was it about me that made you want to interview me today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
